{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acffa06a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6972bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Step 1: Define the LSTM Model ---\n",
    "# This class defines our neural network architecture.\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        # The embedding layer converts integer-encoded words into dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM layer processes the sequence of word embeddings\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # The final fully-connected layer maps the LSTM's output to our desired output dimension\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid for binary classification\n",
    "    def forward(self, text):\n",
    "        # text: [batch size, seq len]\n",
    "        \n",
    "        # Pass text through the embedding layer\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded: [batch size, seq len, embedding dim]\n",
    "        \n",
    "        # Pass the embedded sequence through the LSTM\n",
    "        # The 'h' and 'c' are the final hidden and cell states, which we don't need\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "        # We take the final hidden state of the LSTM for classification\n",
    "        # For bidirectional LSTMs, we concatenate the forward and backward final hidden states\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "        # Pass the final hidden state through the fully-connected layer and sigmoid\n",
    "        dense_output = self.fc(hidden)\n",
    "        output = self.sigmoid(dense_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0274b59",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Step 2: Data Preprocessing ---\n",
    "# A small, simple dataset for demonstration\n",
    "data = [\n",
    "    (\"This movie was great!\", 1),\n",
    "    (\"I loved the film.\", 1),\n",
    "    (\"It was an awesome experience.\", 1),\n",
    "    (\"The acting was terrible.\", 0),\n",
    "    (\"I hated every minute of it.\", 0),\n",
    "    (\"This film is a complete waste of time.\", 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee61165",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a vocabulary from the text data\n",
    "all_words = []\n",
    "for sentence, _ in data:\n",
    "    all_words.extend(sentence.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0342a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(all_words)))\n",
    "word_to_idx = {word: i + 1 for i, word in enumerate(vocab)} # Start from 1 to reserve 0 for padding\n",
    "word_to_idx['<pad>'] = 0\n",
    "vocab_size = len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100489f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert text to numerical sequences\n",
    "def text_to_sequence(text, word_to_idx, max_len):\n",
    "    tokens = text.lower().split()\n",
    "    sequence = [word_to_idx.get(token, 0) for token in tokens] # Use 0 for unknown words\n",
    "    \n",
    "    # Pad the sequence to a fixed length\n",
    "    if len(sequence) < max_len:\n",
    "        sequence += [word_to_idx['<pad>']] * (max_len - len(sequence))\n",
    "    else:\n",
    "        sequence = sequence[:max_len]\n",
    "        \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5267fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Find the maximum sequence length in our dataset\n",
    "max_len = max(len(sentence.split()) for sentence, _ in data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f6d38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a custom PyTorch Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, word_to_idx, max_len):\n",
    "        self.data = data\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, label = self.data[idx]\n",
    "        sequence = text_to_sequence(sentence, self.word_to_idx, self.max_len)\n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor([label], dtype=torch.float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f09933",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the dataset and dataloader\n",
    "dataset = SentimentDataset(data, word_to_idx, max_len)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
