{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acffa06a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6972bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Step 1: Define the LSTM Model ---\n",
    "# This class defines our neural network architecture.\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        # The embedding layer converts integer-encoded words into dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM layer processes the sequence of word embeddings\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # The final fully-connected layer maps the LSTM's output to our desired output dimension\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid for binary classification\n",
    "    def forward(self, text):\n",
    "        # text: [batch size, seq len]\n",
    "        \n",
    "        # Pass text through the embedding layer\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded: [batch size, seq len, embedding dim]\n",
    "        \n",
    "        # Pass the embedded sequence through the LSTM\n",
    "        # The 'h' and 'c' are the final hidden and cell states, which we don't need\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "        # We take the final hidden state of the LSTM for classification\n",
    "        # For bidirectional LSTMs, we concatenate the forward and backward final hidden states\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "        # Pass the final hidden state through the fully-connected layer and sigmoid\n",
    "        dense_output = self.fc(hidden)\n",
    "        output = self.sigmoid(dense_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0274b59",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Step 2: Data Preprocessing ---\n",
    "# A small, simple dataset for demonstration\n",
    "data = [\n",
    "    (\"This movie was great!\", 1),\n",
    "    (\"I loved the film.\", 1),\n",
    "    (\"It was an awesome experience.\", 1),\n",
    "    (\"The acting was terrible.\", 0),\n",
    "    (\"I hated every minute of it.\", 0),\n",
    "    (\"This film is a complete waste of time.\", 0)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
