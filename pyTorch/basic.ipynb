{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209e490",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a10636",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    # The __init__ function sets up the layers of our network.\n",
    "    def __init__(self):\n",
    "        # Always call the parent class's constructor first.\n",
    "        super(SimpleNet, self).__init__()\n",
    "        \n",
    "        # This is our first layer. It takes 1 number as input and outputs 5 numbers.\n",
    "        # Think of it as a layer of 5 \"neurons\" that process the input.\n",
    "        self.hidden_layer = nn.Linear(1, 5)\n",
    "        \n",
    "        # This is our final layer. It takes the 5 numbers from the hidden layer\n",
    "        # and turns them into a single number for our final prediction.\n",
    "        self.output_layer = nn.Linear(5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de4889",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# The forward function is where the data flows through the network.\n",
    "    def forward(self, x):\n",
    "        # First, the data goes through the hidden layer.\n",
    "        # We then apply a \"ReLU\" activation function, which just makes any negative numbers zero.\n",
    "        # This adds non-linearity, which is important for complex problems.\n",
    "        x = torch.relu(self.hidden_layer(x))\n",
    "        \n",
    "        # Finally, the result from the hidden layer goes through the output layer.\n",
    "        # This gives us our final prediction.\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7636f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Step 2: Prepare the Data ---\n",
    "# Let's create a simple dataset for our network to learn from.\n",
    "# The network will learn the simple relationship: y = 2x + 1\n",
    "x_data = torch.randn(100, 1) * 10 # 100 random numbers for our input\n",
    "y_data = 2 * x_data + 1 + torch.randn(100, 1) * 2 # Create labels with some random \"noise\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4086fd7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Step 3: Set up the Training Process ---\n",
    "# Create an instance of our network blueprint.\n",
    "model = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f64345",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We need a way to measure the \"error\" or \"loss\" of our model's predictions.\n",
    "# Mean Squared Error (MSE) is a good choice for this type of problem.\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86dea18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We need an \"optimizer\" to help the network adjust its internal numbers (weights)\n",
    "# to reduce the error. The Adam optimizer is a very common and effective one.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
